{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "FjBmfeqFfabo"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from random import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.random.random(size=(50, 10))\n",
        "y = np.random.random(size=(50, 1))"
      ],
      "metadata": {
        "id": "xRiN0yCkiYpA"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def relu(x):\n",
        "\n",
        "  return np.maximum(0, x)"
      ],
      "metadata": {
        "id": "wSo0LCx0fj85"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def relu_derivative(x):\n",
        "\n",
        "    return (x > 0).astype(float)  # Gradient of ReLU"
      ],
      "metadata": {
        "id": "k4wte8e5K93X"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_propagation(x, w_1, b_1, w_2, b_2, w_3, b_3, w_4, b_4, w_5, b_5):\n",
        "\n",
        "\n",
        "    # First Layer\n",
        "    z_1 = x @ w_1 + b_1\n",
        "    a_1 = relu(z_1)\n",
        "\n",
        "\n",
        "    # Second Layer\n",
        "    z_2 = a_1 @ w_2 + b_2\n",
        "    a_2 = relu(z_2)\n",
        "\n",
        "\n",
        "    # Third Layer\n",
        "    z_3 = a_2 @ w_3 + b_3\n",
        "    a_3 = relu(z_3)\n",
        "\n",
        "\n",
        "    # Fourth Layer\n",
        "    z_4 = a_3 @ w_4 + b_4\n",
        "    a_4 = relu(z_4)\n",
        "\n",
        "\n",
        "    # Output Layer (Regression Output)\n",
        "    y_pred = a_4 @ w_5 + b_5\n",
        "\n",
        "    return z_1, a_1, z_2, a_2, z_3, a_3, z_4, a_4, y_pred"
      ],
      "metadata": {
        "id": "-jpjQ5BmfwDr"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def backward_propagation(lr, w_1, b_1, w_2, b_2, w_3, b_3, w_4, b_4, w_5, b_5, x, y, y_pred, z_1, a_1, z_2, a_2, z_3, a_3, z_4, a_4):\n",
        "\n",
        "    \"\"\" Computes backward propagation to update weights and biases \"\"\"\n",
        "\n",
        "    # Compute Loss Gradient (Mean Squared Error Loss)\n",
        "    m = y.shape[0]\n",
        "    dL_dy_pred = (2 / m) * (y_pred - y)  # Derivative of MSE Loss\n",
        "\n",
        "    # Gradient for Output Layer\n",
        "    dw_5 = a_4.T @ dL_dy_pred\n",
        "    db_5 = np.sum(dL_dy_pred, axis=0, keepdims=True)\n",
        "\n",
        "    # Fourth Layer\n",
        "    dL_dz4 = dL_dy_pred @ w_5.T * relu_derivative(z_4)\n",
        "    dw_4 = a_3.T @ dL_dz4\n",
        "    db_4 = np.sum(dL_dz4, axis=0, keepdims=True)\n",
        "\n",
        "    # Third Layer\n",
        "    dL_dz3 = dL_dz4 @ w_4.T * relu_derivative(z_3)\n",
        "    dw_3 = a_2.T @ dL_dz3\n",
        "    db_3 = np.sum(dL_dz3, axis=0, keepdims=True)\n",
        "\n",
        "    # Second Layer\n",
        "    dL_dz2 = dL_dz3 @ w_3.T * relu_derivative(z_2)\n",
        "    dw_2 = a_1.T @ dL_dz2\n",
        "    db_2 = np.sum(dL_dz2, axis=0, keepdims=True)\n",
        "\n",
        "    # First Layer\n",
        "    dL_dz1 = dL_dz2 @ w_2.T * relu_derivative(z_1)\n",
        "    dw_1 = x.T @ dL_dz1\n",
        "    db_1 = np.sum(dL_dz1, axis=0, keepdims=True)\n",
        "\n",
        "    # Update Weights and Biases\n",
        "    w_1 -= lr * dw_1\n",
        "    b_1 -= lr * db_1\n",
        "\n",
        "    w_2 -= lr * dw_2\n",
        "    b_2 -= lr * db_2\n",
        "\n",
        "    w_3 -= lr * dw_3\n",
        "    b_3 -= lr * db_3\n",
        "\n",
        "    w_4 -= lr * dw_4\n",
        "    b_4 -= lr * db_4\n",
        "\n",
        "    w_5 -= lr * dw_5\n",
        "    b_5 -= lr * db_5\n",
        "\n",
        "    return w_1, b_1, w_2, b_2, w_3, b_3, w_4, b_4, w_5, b_5"
      ],
      "metadata": {
        "id": "a5XqgAcofzxg"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(x, y, lr=0.001, n_epochs=10):\n",
        "    \"\"\" Trains the model using forward and backward propagation \"\"\"\n",
        "\n",
        "    # Get the number of samples & features\n",
        "    n, m = x.shape\n",
        "    n_neurons = 4\n",
        "\n",
        "    # Initialize weights and biases\n",
        "    w_1 = np.random.randn(m, n_neurons) * 0.01\n",
        "    b_1 = np.zeros((1, n_neurons))\n",
        "\n",
        "    w_2 = np.random.randn(n_neurons, n_neurons) * 0.01\n",
        "    b_2 = np.zeros((1, n_neurons))\n",
        "\n",
        "    w_3 = np.random.randn(n_neurons, n_neurons) * 0.01\n",
        "    b_3 = np.zeros((1, n_neurons))\n",
        "\n",
        "    w_4 = np.random.randn(n_neurons, n_neurons) * 0.01\n",
        "    b_4 = np.zeros((1, n_neurons))\n",
        "\n",
        "    w_5 = np.random.randn(n_neurons, 1) * 0.01\n",
        "    b_5 = np.zeros((1, 1))\n",
        "\n",
        "\n",
        "    # Training Loop\n",
        "    for epoch in range(n_epochs):\n",
        "\n",
        "        # Forward Propagation\n",
        "        z_1, a_1, z_2, a_2, z_3, a_3, z_4, a_4, y_pred = forward_propagation(x, w_1, b_1, w_2, b_2, w_3, b_3, w_4, b_4, w_5, b_5)\n",
        "\n",
        "        # Compute Loss\n",
        "        loss = np.mean((y_pred - y) ** 2)\n",
        "\n",
        "        # Backward Propagation\n",
        "        w_1, b_1, w_2, b_2, w_3, b_3, w_4, b_4, w_5, b_5 = backward_propagation(lr, w_1, b_1, w_2, b_2, w_3, b_3, w_4, b_4, w_5, b_5, x, y, y_pred, z_1, a_1, z_2, a_2, z_3, a_3, z_4, a_4)\n",
        "\n",
        "        if epoch % 2 == 0:\n",
        "            print(f\"Epoch {epoch}: Loss = {loss:.4f}\")\n",
        "\n",
        "    return w_1, b_1, w_2, b_2, w_3, b_3, w_4, b_4, w_5, b_5\n"
      ],
      "metadata": {
        "id": "D3PxXERC-f-B"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w_1, b_1, w_2, b_2, w_3, b_3, w_4, b_4, w_5, b_5 = train_model(X, y)"
      ],
      "metadata": {
        "id": "ZYv6_TsmgmBi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5a5daa8-fab7-4e30-e123-c4e5d352026b"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: Loss = 0.3902\n",
            "Epoch 2: Loss = 0.3876\n",
            "Epoch 4: Loss = 0.3850\n",
            "Epoch 6: Loss = 0.3824\n",
            "Epoch 8: Loss = 0.3798\n"
          ]
        }
      ]
    }
  ]
}